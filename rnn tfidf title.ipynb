{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=X.copy()\n",
    "messages.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 18284 / 18285\r"
     ]
    }
   ],
   "source": [
    "lr = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    print(\"Status: %s / %s\" %(i, len(messages)), end=\"\\r\")\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [lr.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'house dem aide even see comey letter jason chaffetz tweeted'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_title, X_test_title, y_train_title, y_test_title = train_test_split(corpus, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_vocab_title = 1500\n",
    "\n",
    "# Create TF-IDF of title\n",
    "tfidf_title = TfidfVectorizer(stop_words=stopwords.words('english'),ngram_range=(1,3), max_features=max_vocab_title)\n",
    "\n",
    "sparse_tfidf_train_title = tfidf_title.fit_transform(X_train_title)\n",
    "\n",
    "X_tfidf_train_title = sparse_tfidf_train_title.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                96064     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 97,121\n",
      "Trainable params: 97,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "59/59 [==============================] - 1s 15ms/step - loss: 0.6704 - accuracy: 0.6758 - recall: 0.7963 - val_loss: 0.6200 - val_accuracy: 0.7785 - val_recall: 0.9976\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 0s 8ms/step - loss: 0.5590 - accuracy: 0.8485 - recall: 0.9880 - val_loss: 0.5353 - val_accuracy: 0.8978 - val_recall: 0.9638\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - 0s 8ms/step - loss: 0.5040 - accuracy: 0.9151 - recall: 0.9712 - val_loss: 0.5248 - val_accuracy: 0.9067 - val_recall: 0.9605\n",
      "Epoch 4/10\n",
      "59/59 [==============================] - 0s 8ms/step - loss: 0.4839 - accuracy: 0.9324 - recall: 0.9684 - val_loss: 0.5249 - val_accuracy: 0.9070 - val_recall: 0.9404\n",
      "Epoch 5/10\n",
      "59/59 [==============================] - 0s 8ms/step - loss: 0.4696 - accuracy: 0.9410 - recall: 0.9673 - val_loss: 0.5297 - val_accuracy: 0.9087 - val_recall: 0.9420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e871ec8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_title = tf.keras.Sequential()\n",
    "model_title.add(tf.keras.layers.Dense(64,input_shape=(max_vocab_title,),activation='relu'))\n",
    "model_title.add(tf.keras.layers.Dropout(0.2))\n",
    "model_title.add(tf.keras.layers.Dense(16,activation='relu'))\n",
    "model_title.add(tf.keras.layers.Dropout(0.2))\n",
    "model_title.add(tf.keras.layers.Dense(1,activation='relu'))\n",
    "model_title.add(tf.keras.layers.Activation('sigmoid'))\n",
    "model_title.summary()\n",
    "model_title.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy', 'Recall'])\n",
    "\n",
    "batch_size = 200\n",
    "max_epochs = 10\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model_title.fit(X_tfidf_train_title, y_train_title, batch_size = batch_size, epochs = max_epochs, callbacks=[early_stopping], validation_split = 0.2 , shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tfidf_test_title = tfidf_title.transform(X_test_title)\n",
    "\n",
    "X_tfidf_test_title = sparse_tfidf_test_title.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.5198 - accuracy: 0.9163 - recall: 0.9627\n"
     ]
    }
   ],
   "source": [
    "model_title_loss, model_title_accuracy, model_title_recall = model_title.evaluate(X_tfidf_test_title, y_test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.51984041929245, 0.9163248538970947, 0.9626818299293518)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_title_loss, model_title_accuracy, model_title_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_title.save('rnn_title_tfidf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(tfidf_title, open('tranform.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
